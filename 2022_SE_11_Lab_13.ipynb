{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a06ihLRCGZYL"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Load Dataset (Update with correct path if needed)\n",
        "try:\n",
        "    data = np.loadtxt('/content/ANN_SLP.ipynb.csv', delimiter=',')  # Update path if necessary\n",
        "    X, y = data[:, :-1], data[:, -1]\n",
        "except Exception as e:\n",
        "    print(\"Dataset not found, generating mock data.\")\n",
        "    from sklearn.datasets import make_classification\n",
        "    X, y = make_classification(n_samples=100, n_features=5, random_state=42)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1EplUUVXISpv",
        "outputId": "1887d5c1-ee95-4e60-ab26-54f10b6db8b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset not found, generating mock data.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Preprocess Data (Standardize)\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)"
      ],
      "metadata": {
        "id": "5syUeennIOH2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Split Data into Training and Testing Sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "1jxpox3BIlcb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Define Step Function\n",
        "def step_function(x):\n",
        "    return 1 if x >= 0 else 0"
      ],
      "metadata": {
        "id": "209vIER_IoMG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 5: Train Perceptron\n",
        "def train_perceptron(X, y, learning_rate=0.1, epochs=100):\n",
        "    num_features = X.shape[1]\n",
        "    weights = np.zeros(num_features + 1)  # +1 for bias\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        total_error = 0\n",
        "        for i in range(len(X)):\n",
        "            x_with_bias = np.insert(X[i], 0, 1)  # Add bias\n",
        "            weighted_sum = np.dot(weights, x_with_bias)\n",
        "            y_pred = step_function(weighted_sum)\n",
        "            error = y[i] - y_pred\n",
        "            weights += learning_rate * error * x_with_bias\n",
        "            total_error += abs(error)\n",
        "\n",
        "        if total_error == 0:\n",
        "            break\n",
        "\n",
        "    return weights"
      ],
      "metadata": {
        "id": "aK9oFhwsIsbH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 6: Train the Model\n",
        "weights = train_perceptron(X_train, y_train)"
      ],
      "metadata": {
        "id": "FBepQExZIyEa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Step 7: Make Predictions\n",
        "def predict(X, weights):\n",
        "    predictions = []\n",
        "    for i in range(len(X)):\n",
        "        x_with_bias = np.insert(X[i], 0, 1)\n",
        "        weighted_sum = np.dot(weights, x_with_bias)\n",
        "        predictions.append(step_function(weighted_sum))\n",
        "    return np.array(predictions)"
      ],
      "metadata": {
        "id": "lgRdPikxI4-u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 8: Evaluate the Model\n",
        "y_pred = predict(X_test, weights)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy: {accuracy:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kz5S_t7nI8u_",
        "outputId": "0424b64a-f43f-463b-fc5b-9d1c8e44d254"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 1.00\n"
          ]
        }
      ]
    }
  ]
}